[/==============================================================================
    Use, modification and distribution is subject to the Boost Software License,
    Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
    http://www.boost.org/LICENSE_1_0.txt)
===============================================================================/]



[section Closure Execution Engines]

[def __msvc__ MSVC]
[def __stlport__ [@http://sourceforge.net/projects/stlport STLport]]


__boost_afio__ can be viewed as an an asynchronous, chainable, batch executable, closure 
execution engine. While this is a somewhat intimidating characterization,the core concept 
of such an engine is actually fairly straight forward.  The next few sections will attempt 
to clear up exactly what this title means, and to give the user a reference for how it 
can be used when compared to some better known implementations. If you are already 
familiar with what an synchronous, chainable, batch executable, closure execution engine 
is, then you may wish to skip to section  XXX


[heading: Closure Execution Engines]


First, it will be useful to introduce some core concepts before moving on to a more indepth 
look at closure execution engines. Let us begin with a simple definition of a closure. 
Wikipedia.org suplies a clear definition:


“In programming languages, a closure (also lexical closure or function closure) is a function or 
reference to a function together with a referencing environment—a table storing a reference to 
each of the non-local variables (also called free variables or upvalues) of that function.[1] 
A closure—unlike a plain function pointer—allows a function to access those non-local variables 
even when invoked outside of its immediate lexical scope.” (Footnote or link)


Basically a closure can be thought of as an anonymous function object capable of capturing local
state for use outside of its lexical scope. In C++ a closure strongly implies the use of a lambda 
expression, though there are other ways to create closures. C++ is a very powerful and flexible 
language, and how a closure is created is of less importance than what it does. If you are 
unfamiliar with lambdas or closures in C++ we encourage you to do some research on the subjects 
before continuing, as a basic understanding of these core concepts will be important moving 
forward. 


Now that we have a basic idea of what a closure is, consider the role that a closure execution 
engine plays: to execute the closures supplied to it.  How the engine works is largely driven by 
its design goals and performance criteria. A simplistic analogy is to consider a rocket engine 
and a combustion engine found in an automobile: both engines fill similar roles (vehicle 
propulsion) but are quite different in how they work and perform the basic task of vehicle 
locomotion, ie their design goals and performance criteria are completely different. So a closure 
execution engine is a fairly generic term, and says nothing about the implementation.


Earlier we defined AFIO as a specialized type of closure execution engine: an asynchronous, 
chainable, batch executable, closure execution engine. Now that we have a basic understanding of 
what a closure execution engine is, understanding that it is asynchronous, chainable and batch 
executable aren't quite as daunting. 


*Asynchonous: closures can be completed in an asynchronous fashion, rather than with traditional 
sequential blocking behavior. 
*Chainable: closures can be chained together, so that dependent 
operations can be scheduled for execution after the closures they depend on have been completed. 
*Batch Executable: A group of closures can be  scheduled for execution. 


So, AFIO can execute batches of sequential closures for asynchronous completion. What this means 
for users is that it is possible to break up a series of sequential tasks into a closures, and 
have AFIO schedule them for asynchronous completion while the main thread is free to complete 
other work. 


Because AFIO's main focus is on file I/O, we've selected an example where the I/O operations are 
the source of blocking behavior. Take a simple example of reading in an array of integers from a 
file, and then preforming some calculations with the entries of the array. Traditionally a user 
would be required to open and read the contents of the entire array before beginning 
computations. With AFIO it is possible to schedule the file to be opened, then for each integer 
to be read into memory, and have a calculation scheduled for immediate execution after the 
specific data it required finished being read from file into memory. Finally, the file can be 
scheduled to be closed after all operations have been completed, all without waiting for the 
entire contents of the file to finish being read. This is a trivial example, but it illustrates 
very well how closures can be used and scheduled to improve performance. These benefits are not 
limited to file I/O, and the closures can be virtually any non-blocking task and still improve 
performance over sequential execution.


[*Traditonal example:]

``` 
#include <iostream>
#include <fstream>

int main()
{
    
    const int ary_size = 10;

    // set up a file to read from
    std::ofstream out_file("somefile.dat", std::ios::binary);
    for (int i = 0; i < ary_size; ++i)
    {
        out_file.write(reinterpret_cast<const char*>(&i), sizeof(i));
    }
    out_file.close();
    
    //setup an array of integers
    int ary[ary_size];
    //file open
    std::ifstream file("somefile.dat");

    //read in ints to ary
    if (file)
    {
        for (int i = 0; i < ary_size; ++i)
        {
            file.read((char*) &ary[i], sizeof(ary[i]));
        }
        //close file
        file.close();

        
        //do some work with the array of ints
        for (int i = 0; i < ary_size; ++i)
            ary[i] *= 2;
    } 

    //verify the out put is as expected: "0, 2, 4, 6, 8, 10, 12, 14, 16, 18"
    for (int i = 0; i < ary_size; ++i)
    {
        std::cout << ary[i];
        if(i == ary_size-1)
            std::cout << std::endl;
        else
            std::cout << ", ";
    }

    return 0;
}
```


[*AFIO Example:]

``` 
#include <iostream>
#include <fstream>
#include <vector>
#include <boost/afio/afio.hpp>

int main()
{
    const int ary_size = 10;

    //set up a file to read from
    std::ofstream out_file("somefile.dat", std::ios::binary);
    for (int i = 0; i < ary_size; ++i)
    {
        out_file.write(reinterpret_cast<const char*>(&i), sizeof(i));
    }
    out_file.close();
    

    //set up the afio dispatcher
    auto dispatcher = boost::afio::make_async_file_io_dispatcher();

    //set up an array to hold our integers
    int ary[ary_size];

    //schedule the file open
    auto opened_file = dispatcher->file(boost::afio::async_path_op_req("somefile.dat", boost::afio::file_flags::Read));

    //set up vectors for the individual read operations, and the work on each integer
    std::vector<boost::afio::async_io_op> read_ops(ary_size);
    std::vector<std::function<void()>> vec_func(ary_size);
    for (int i = 0; i < ary_size; ++i)
    {
         read_ops[i] = dispatcher->read(boost::afio::async_data_op_req<int>(opened_file, &ary[i], sizeof(int), i*sizeof(int)));
        
         vec_func[i] = std::bind([](int* a){ *a *= 2 ; }, &ary[i]);
    }
    
    // schedule the work to be done after reading in an integer
    auto work = dispatcher->call(read_ops, vec_func);
    
    //schedule the file to be closed after reads are finished
    auto closed_file = dispatcher->close(dispatcher->barrier(read_ops).front());
    
    // make sure work has completed before trying to print data from the array
    boost::afio::when_all(work.second.begin(), work.second.end()).wait();
    
    //verify the out put is as expected: "0, 2, 4, 6, 8, 10, 12, 14, 16, 18"
     for (int i = 0; i < ary_size; ++i)
    {
        std::cout << ary[i];
        if(i == ary_size-1)
            std::cout << std::endl;
        else
            std::cout << ", ";
    }

    return 0;
}
```


While the example above is a bit trivial, it serves as a good example of basic functionality. for a small
array of only 10 integers, this is a bit of overkill, but for a very large array of data, the benefits may be
quite large. Additionally the examples show that the new code is still readable, concise, and clear, all while
giving the user the benefit of asynchronous execution of discrete tasks. It is important to understand that
not every set of tasks are readily translatable into a series of asynchronous operations, but for those that
are AFIO provides a powerful tool for scheduling and executing them. 


[heading How AFIO Relates to Exisiting Closure Execution Engines]

Proposals N3562 and its predecessor N3378  outline an executor library based on internal 
implementations of similar facilities at Google and Microsoft. AFIO is based, in part, on some of 
the concepts and rationale from those papers. So how does AFIO relate to the new facilities 
proposed in those papers?


[*Abstract Executor]


`class executor` has an AFIO equivalent in `class async_file_io_dispatcher_base` (hence refereed to as “ `dispatcher` ” for brevity). The two classes are similar in that both are abstract base classes, and both are largely responsible for scheduling closures for execution. Technically `dispatcher` does not have an analog for `executor::add(std::function<void()>)` as the methods it uses to schedule closures for execution do not accept `std::function<void()>`. There is, however, a mechanism for adding closures for future execution: `completion()`, and its convenience wrapper `call()`. Both of these functions allow the user to schedule closures for future execution, though in slightly different ways. Additionally dispatcher provides several specialized versions of `executor::add()` in the form of specialized file operations (eg. `file, open, close, dir, rmdir`, etc.), which schedule these tasks for future execution. 

Because AFIO is designed to be chainable and batch executable, its functions tend to return futures, or vectors of futures, to obtain results from currently scheduled tasks, or chain a series of tasks together. While there is nothing in N3562 to forbidding such behavior, it doesn't explicitly call for it either.  


[*Executor Factory]


AFIO does provide an executor factory function `make_async_file_io_dispatcher()`, which is responsible for creating new dispatchers.  Unlike the proposed factory functions in N3378, there is no distinction made between factories. This is similar to `singleton_inline_executor()` from N3562, except `make_async_file_io_dispatcher()` is not guaranteed to return a singleton. 


[*Scheduled Executors]


AFIO has no support for the scheduled_executor class. As a result it also does not support `add_at()` or `add_after()` functions. 

[*Default Executors]


AFIO has no notion of a default executor, and hence the functions `default_executor()` and `set_default_executor()` proposed in N3562 have no analog in AFIO.


[*Concrete Executor classes]


These classes are the derived classes of the abstract base classes `executor` and `async_file_io_dispatcher_base`. N3562 proposes two such derived classes, a `loop_executor`, and a `serial_executor`. AFIO does not support either of these paradigms, but rather has two derived classes that handle different operating systems (Windows and POSIX) that are created through the factory function `make_async_file_io_dispatcher_base()`. 


AFIO does have access to a `loop_executor` analog, via Boost.ASIO. `boost::asio::io_service` can be thought of as a sort of `loop_executor`, and has many of the member functions called for in N3562. 


[*Threadpools]


AFIO's aysnc_file_io_dispatcher_base uses a threadpool rather than other executors to handle closures, and as a result is very similar to N3562's class thread_pool.  AFIO provides the thead_source and std_thread_pool classes to give  dispatcher similar functionality the N3562's class thread_pool. 


[*Synchronization Mechanisms]


AFIO provides several methods to synchronize results for asynchronous operations:
	1. barrier() : synchronize based on a group of ops completing. 
	2. when_all() : synchronize based on an entire group of ops completing. 
	3. when_any() : synchronize based on the first completion from a group of ops

[endsect] [/ end of Closure Execution Engines]
