[/============================================================================
  Boost.AFIO

  Use, modification and distribution is subject to the Boost Software License,
  Version 1.0. (See accompanying file LICENSE_1_0.txt or copy at
  http://www.boost.org/LICENSE_1_0.txt)
=============================================================================/]

[section:quickstart Quick Start Tutorial]

So why bother with __boost_afio__? What's wrong with the STL iostreams and Filesystem?

The answer is that there is nothing wrong with either for 95% of use cases. Performance
of both is pretty good in fact most of the time __dash__ which actually isn't that
surprising as C++ is a pay-for-what-you-use systems language. If however you have ever
experienced any of these problems, then AFIO may be useful to you:

# Your spinning magnetic rust hard drive goes bananas when some routine in your code
  tries to do something to storage, and latency per op starts heading into the seconds range.

# Your super fast SSD which is supposed to be delivering hundreds of thousands of ops/sec
  is barely managing a tenth of its supposed ability with your code. After reading about the
  importance of high queue depth to maximising performance from SSDs, you try opening many
  handles to the same file and firing an army of thread pool workers at the problem to try
  and increase queue depth, but your performance actually drops over the single threaded case.

# Your code has to interact with a regularly changing filesystem and not get weird race errors e.g. you
  try to create a new file in path /X/Y/Z, but some other program has just renamed directory /X/Y
  to /A/B in the time between you deciding on /X/Y/Z and getting round to it.

# Your code keeps file handles open a long time in a place where others might delete or rename
  them, including any part of the directory hierarchy preceding the file.
  
# Deleting directory trees randomly fails on Microsoft Windows for no obvious reason.

# Your code needs to read and write files concurrently to other code without resorting to
  shared memory region tricks e.g. if the files reside on a Samba or NFS network shared drive.

# Your CPU needs to be doing more useful work instead of copying memory to and from disc i/o
  buffers. As great as the STL iostream buffering is, unless disabled it doubles the LL cache
  pressure on your CPU, evicting other more useful data. The STL iostreams design almost certainly won't
  allow the kernel use VM tricks to directly busmaster DMA from its buffers to the hard drive, so
  the kernel will have to copy those buffers a third time. That means that for every 1Kb you read
  or write you are evicted, as a minimum, 3Kb from the LL caches in your CPU, all of which must be
  refilled with more useful data later.

# Your code wants to experience various filing system features identically across platforms which
  also work on shared Samba and NFS network drives, such as:
  * Deleting and renaming open files.
  * Files having unique inode values.
  * POSIX timestamping of last accessed, last modified, last status changed and created.
  * File extent management and traversal.
  * Explicitly documented filing system race guarantees.
  * Interrogation of filing system characteristics, devices and mount points.
  * Ten million item directories, or more. We have tested twenty five million item directories on NTFS
    and ext4 and performance was actually tolerable with under a second pause. Ten million item directories is plenty fast, and
    one million item directories you won't notice over a ten item directory. Note that your GUI file
    explorer will very likely hang on ten million item directories, indeed so do most command line tools.
  * Exclusive lock files (manually operated support already there, async support coming in v1.5).
  * File change monitoring (coming in v1.5).
  * File byte range advisory locking (coming in v1.5).
 
You'll probably note that these are POSIX file semantics. On Microsoft Windows, AFIO directly uses
the NT kernel API and can therefore achieve reasonably accurate POSIX file semantics, at the cost of
no support for DOS feature emulation like short names or case insensitivity or drive letters. Going directly to the
NT kernel makes filing system performance on Windows both much faster and a close experience
to coding for POSIX, this saves you time and money and produces a much more consistent experience
for your users.

A surprising amount of production code out there is highly unreliable when used on a filing
system experiencing rapid change, or even just a filing system mounted on a network. In many
ways it is when your code needs to ["grow up] from assuming a never changing static filesystem
into a more realistic model is when you ought to reach AFIO which has hopefully abstracted away
all those tedious platform specific and filing system specific quirks for you.

The quick start tutorial is broken into two sections:

# A step by step workshop in building an AFIO program which can concurrently read/write an indexed
blobstore. This workshop was presented at CppCon 2015 and the slides and materials can be found
at <TBD LINK>. Issues covered:
 * Files being renamed, created or deleted by others when you are using them.
 * Directories being renamed, created or deleted by others when you are using them.
 * Windows only: how to correctly delete a directory tree on Windows, and why almost every
implementation of directory tree deletion for Windows is unreliable.
 * Techniques for avoiding filing system races:
   * Inode checking
   * Lock files
   * Atomic renames
   * Never use absolute paths
   * Windows only: use GUID instead of path lookup.
# Real world sample mini-programs written using AFIO which show various filing system algorithms
and strategies in action.
 * Hello world
 * Concatenating files
 * Atomicity on the filing system
 * Races on the filing system
 * Find regular expression in files in directory tree

[section:workshop Step by step workshop building an AFIO-based key-value store]

[section:naive World's simplest named blob store in STL iostreams]

Let us imagine your application needs to persist some state across executions, and that that state
is merely a collection of key-value items e.g.

* "dog" => "I am a dog"
* "cat" => "I am a cat"
* "horse" => "I am a horse"
* ...

Let us imagine that you write the following low level public interface for this key-value store:

[workshop_naive_interface]

The macros for the monad and afio namespaces are to enforce a ['hard] version dependency. By aliasing
into the `BOOST_AFIO_V2_NAMESPACE`, you are creating a specific dependency on v2 of the AFIO ABI. AFIO
ships some still supported previously used ABI versions of itself in every version, so by pinning yourself
to v2 you mean v2 and nothing but v2. If you just wanted the current AFIO, simply alias into namespace
`boost::afio` as with other libraries, this picks up whatever the current configuration and version of
AFIO is.

The `monad<>` comes from __boost_monad__ which is a dependency of __boost_afio__. It has an identical
API to `std::future<>` or more rather `boost::future<>`, so simply treat it as an always ready future.
`monad<>` here can return a shared pointer to the iostream, or empty (item not found), or an error, or
an exception as you can see in this example use case:

[workshop_use_naive]

A perfectly straightforward and simple way of implementing `data_store` using pure C++ STL is this:

[workshop_naive]

Note that `monad<T>` implicitly consumes any `T`, `std::error_code`, `std::exception_ptr` or `empty`, hence
the ability to return any of those directly.

This very simple solution assumes that the key-value store is a directory in the current path called ["store] and
that each key-value item is simply a file called the same name as the key name.

[table:conditions This solution will perform pretty well and is perfectly fine under these conditions:
[[][Condition]]
[[__cross__][On Microsoft Windows you don't place the store deep in a directory hierarchy and you use only short key names.]]
[[__cross__][No third party thread or process will rename the location of the store during use.]]
[[__cross__][The size of the values being read and written is small.]]
[[__cross__][Only one thread or process will ever interact with the key-value store at a time.]]
[[__cross__][You don't care what happens if your process unexpectedly exits during a modify.]]
[[__cross__][Maximum performance isn't important to you.]]
[[__cross__][You don't care what happens under power loss.]]
[[__cross__][You don't need to update more than one key-value at once.]]
]

[endsect] [/ naive]

[section:naive_afio World's simplest named blob store in AFIO]

Let's see the same thing as in the last section, but written using AFIO. First, the interface is identical
to before, just with different private member variables:

[workshop_naive_afio_interface]

We now have a `dispatcher_ptr` and a `handle_ptr` to the store directory which is created/opened during
construction of `data_store`. Instead of constructing to a filesystem path, we now construct to an AFIO path.
Note that AFIO paths are always either absolute or relative to some open file handle, and therefore in this
situation at the point of construction the current working directory will be fetched and an absolute path
constructed. This differs from filesystem which passes through relative paths and lets the OS resolve from
the current working directory __dash__ AFIO does it this way as the current working directory setting at some
later point of asynchronous execution is inherently unpredictable. As the `data_store` interface is identical,
so is the use case from the previous page. The implementation is rather different however:

[workshop_naive_afio1]

This is a good bit longer and looks more complex, however there is already a big if not obvious gain: third party
processes can happily rename and move around the store directory once it has been opened and this implementation
will work perfectly. This is because we open a handle to the store directory in the `data_store` constructor,
and thereafter use that open handle as a base location for all leaf path operations. Except on OS X which currently
doesn't support the POSIX race free filesystem extensions, that makes all data store operations invariant to
store path mutation on all supported platforms.

Another big gain is we are now using memory mapped files for the lookup which avoids any memory copying, and
there is a hint we are also avoiding memory copies in the write too.

You will also note that in the constructor, we specify a URI to `make_dispatcher()`. This lets you open
different kinds of filesystem e.g. ZIP archives, HTTP sites etc. AFIO currently doesn't provide backends except
for `file:///` i.e. the local filesystem, however future versions will. Note that a dispatcher can force on or off
`file_flags` for all operations scheduled against that dispatcher __dash__ here we force mask out any attempt to
open anything for writing if we are opening the store read-only.

Finally, you may wonder why we only use `current_dispatcher_guard` once in the constructor. This is because
if AFIO can deduce the dispatcher to use from any precondition you supply, you need not set the thread local
dispatcher. As the opening of the store directory is done as an absolute path lookup and therefore takes no
inputs specifying a dispatcher, you need to set the current dispatcher in that one situation alone.

The remaining magic is in the custom iostreams implementations `idirectstream` and `odirectstream`:

[workshop_naive_afio2]

These are not hugely conforming iostreams implementations in order to keep them brief for the purposes of this
tutorial. The read stream is very straightforward, we simply have `streambuf` directly use the memory map of
the file.

The write stream is a bit more complicated: we fill a 4Kb page and ['asynchronously] write it out
using a page stealing friendly algorithm which in the usual case means the kernel simply takes possession of
the 4Kb page as-is with no memory copying at all. At some future point it will be flushed onto storage. The
reason this works is thanks to the special STL allocator `utils::file_buffer_allocator<>` which returns whole kernel
page cache pages. Most kernels will mark whole pages scheduled for write with copy-on-write behaviour such that
they can be safely DMAed by the kernel DMA engine as any subsequent write will cause a copy, so because we never write to the page between the write
request and freeing the page, most kernels simply transfer ownership of the page from the user space process
to the file page cache with no further processing. Hence the asynchronous write of the page tends to complete very
quickly __dash__ indeed far faster than copying 4Kb of memory and often quicker than the time to fill another page, and hence we wait for any previous write to
complete before scheduling the next write in order to report any errors which occurred during the write.

This is the first use of asynchronous i/o in this tutorial. AFIO provides a custom `future<T>` type extending
the lightweight monadic futures framework in __boost_monad__, so you get all the
[http://www.drdobbs.com/parallel/improving-futures-and-callbacks-in-c-to/240004255 C++ 1z Concurrency TS extensions],
[http://blogs.msdn.com/b/vcblog/archive/2014/11/12/resumable-functions-in-c.aspx C++ 1z coroutines support] and
[http://www.boost.org/doc/html/thread/synchronization.html#thread.synchronization.futures Boost.Thread future extensions] in the AFIO custom `future<>`. There are also many
additional extensions beyond what Boost.Thread or the Concurrency TS provides including foreign future composable waits, and you can see one of those extensions in
the Concurrency TS continuation (the `.then()`) in `sync()` which takes a const lvalue ref to the future to avoid consuming it.
The continuation simply pins a shared pointer to the buffer to the completion of `async_write()`, thus ensuring
the lifetime of the buffer extends past the write operation.

[table:conditions This solution will perform as well as the iostreams implementation for small values and is fine under these conditions:
[[][Condition]]
[[__tick__] [On Microsoft Windows you can place the store deep in a directory hierarchy and use long key names.]]
[[__tick__] [Third party threads and processes can rename the location of the store during use.]]
[[__tick__] [The size of ['all] the values being read at any given time fits into your virtual address space (which is at least 2Gb on 32 bit, 8Tb on 64 bit).]]
[[__cross__][Only one thread or process will ever interact with the key-value store at a time.]]
[[__cross__][You don't care what happens if your process unexpectedly exits during a modify.]]
[[__cross__][Maximum performance isn't important to you.]]
[[__cross__][You don't care what happens under power loss.]]
[[__cross__][You don't need to update more than one key-value at once.]]
]

[endsect] [/ naive_afio]

[section:naive_racy The problems with this naive solution]

ACID

Atomicity: Writes do not atomically appear fully completed to other readers.

Consistency: If a writer process fatal exits during a write, the key value store is corrupted.

Isolation: There is no concurrency ordering control apart from that provided by the operating system.

Durability: Is the key value store always consistent even with random power loss?

[endsect] [/naive_racy]

[section:atomic_updates How to implement atomic value updates and therefore Atomicity, Consistency, Isolation]

[endsect] [/atomic_updates]

[section:durability Implementing Durability and garbage collection]

[endsect] [/durability]

[section:performance What's the performance of this naive key-value store?]

[endsect] [/performance]


[section:deduplication Adding deduplication of identical value storage]

[endsect] [/deduplication]


[endsect] [/ workshop]

[section:mini_programs Example mini-programs written using AFIO]

[section:hello_world Hello World, asynchronously!]

'''<?dbhtml-include href="disqus_identifiers/hello_world.html"?>'''

We might as well jump straight in: on the left is a traditional STL iostreams
implementation with its equivalent in AFIO free functions on the right.

[table:hello_world Traditional STL iostreams and AFIO side by side
  [[[readwrite_example_traditional]][[readwrite_example]]]
]

AFIO is based on future continuations as will be in the standard C++ library from C++ 1z onwards.
Each continuation enforces an order of execution between the asynchronous operations.

What this means is straightforward: in the example above `async_file()` outputs on completion
an open file handle which is fed to `async_truncate()` which on completion feeds the same input
handle to `async_write()` and so on. `async_close()` takes in the open file handle, closes it, and
outputs a null file handle on completion to `async_rmfile()` which can still retrieve its last known
path before being closed.

'''<?dbhtml-include href="disqus_comments.html"?>'''

[endsect] [/hello_world]

[section:file_concat A less toy example: Concatenating files]

'''<?dbhtml-include href="disqus_identifiers/file_concat.html"?>'''

The Hello World example didn't really demonstrate why using AFIO is any better than using
STL iostreams. The final example in this quick start will give a ["real world] and unfortunately
somewhat overwhelming in complexity example of how
AFIO can run rings around STL iostreams (complete with benchmarks!), so as an intermediate
__dash__ and hopefully not as overwhelming __dash__
step here is a simple file copy utility which can also concatenate multiple source files
into a destination file[footnote My thanks to Artur Laksberg, Dev Lead of the Microsoft
Visual C++ Parallel Libraries team, for suggesting this as a good demonstration of an
asynchronous file i/o library. I had been until then quite stumped for an intermediate
quick start example between the first and last examples.].

[filecopy_example]

This example consists of a function called `async_concatenate_files()` which will 
asynchronously append those file paths in ['sources] into the file path in ['dest], with
the `main()` function simply parsing arguments and printing progress every second.
I won't explain the example hugely __dash__ it's pretty straightforward, it simply
parallel reads all source files in `chunk_size` chunks, writing them to their appropriate
offset into the output file. It
is a very good example, incidentally, of why C++11 is like a whole new language over
C++98 even for simple systems programming tasks like this one.

You may note that the temporary buffers for each chunk are allocated using a special
`file_buffer_allocator`. If your operating system allows it, regions of memory returned
by this allocator have the maximum possible scatter gather DMA efficiency possible.
Even if your operating system doesn't allow it, it does no harm to use this allocator
instead of the system allocator.

The chances are that this file copying implementation would be little faster than a naive
implementation however (unless the source files are on different physical devices, in which
case you'd see maximum performance). Some might also argue that firing a consumer producer thread per source
file would be just as easy, albeit that output file position management across threads
might be slightly tricky.

Let us start into where the AFIO implementation starts to deliver real value add: a multiprocess safe log file and
finding which files in a directory tree contain a regular expression.

'''<?dbhtml-include href="disqus_comments.html"?>'''

[endsect] [/file_concat]

[section:atomic_logging Achieving atomicity on the filing system]

'''<?dbhtml-include href="disqus_identifiers/atomic_logging.html"?>'''

__triplegit__, the forthcoming reliable graph database store, ideally needs to allow multiple processes to
concurrently read and write the graph store without mutual exclusion where possible. Each collection of hashes which form
the tree which makes up some given version of the graph is itself a hashed object in the content
addressable store, and you can have multiple named graphs in the graph store which may or may not
share nodes. One problem as with all databases is how to efficiently issue an atomic transaction
which updates multiple graphs simultaneously and atomically when there is the potential of concurrent
writers also trying to issue write transactions which may, or may not, cause conflict with other
transactions.

The really naive solution is to keep a single lock file which is created using O_EXCL i.e. fail
if you didn't create the file for the entire graph store. This serialises
all transactions, and therefore eliminates any problems with updates clashing. This is usefully
well supported by all operating systems, and by NFS and Samba.

A less naive solution is to keep one lock file per graph, and to
use a multiple lock and backoff strategy to lock the requisite number of graphs for some given
transaction. The big problem with this approach is that you are unfairly penalised for especially large multi-graph transactions over others
with smaller transactions as lock complexity is worse than linear. Nevertheless performance is actually not bad: these are results for my 3.9Ghz i7-3770K workstation
using AFIO to implement the lock file with various numbers of concurrent writers (note that Windows provides magic
flags for telling it about lock files, if not used expect a 40% performance reduction):

[table:lock_file_performance Lock file performance on various operating systems:
  [[writers][lock files][Win8.1 x64 NTFS HD][Win8.1 x64 NTFS SSD][Linux x64 ext4 SSD][FreeBSD 10.1 ZFS SSD]]
  [[1][1][2468][2295][3590][9922]]
  [[2][1][2507][2385][3583][9903]]
  [[4][1][1966][2161][3664][9684]]
  [[8][1][1400][1851][3703][6537]]
  [[16][1][742][602][3833][1251]]
  []
  [[1][8][826][888][1378][2455]]
  [[2][8][508][637][576][917]]
  [[4][8][67][167][417][63]]
  [[8][8][37][117][106][0.55]]
  [[16][8][33][77][26][0.5]]
]

As you can see, Linux does a very good job of O(1) to waiters complexity, but performance on Windows
and FreeBSD collapses once you exceed CPU cores. Also, Windows is sensitive to device block size __dash__ the
hard drive outperforms the SSD because it has 512 byte sectors and the SSD has 4096 byte sectors.
I suspect that internally Windows memcpy()'s in device native sector sizes, or is actually sending
data to physical storage despite us marking this file as temporary. One very striking observation is
how FreeBSD is a full 2.76x the performance of Linux and 4.32x that of Windows.

A much more intelligent way of solving this problem is to figure out which graphs are common across each
of the transactions currently pending, and to strictly order the transactions in the sequence
which maximises throughput without updates clashing.
One way of many distributed writers constructing a shared graph of dependencies is to append messages
into a shared file, and then one can deploy a distributed mutual exclusion algorithm of which those
by Suzuki and Kasami, Maekawa and Ricart and Agrawala are the most famous. This requires the ability
to atomically append to the shared file, something guaranteed on all operating systems, but unfortunately
not guaranteed by NFS nor Samba (though when combined with advisory file locking those do work as
expected, albeit with poor performance). This means that on an all-Windows setup, or if on POSIX and
not using NFS nor Samba, the atomic append method could be valuable, especially as the cost of locking
multiple actors is pretty much the same as locking a single actor so you get eight graphs locked for
the same cost as locking one.

[table:atomic_append_performance Atomic append lock performance on various operating systems:
  [[writers][lock files][Win8.1 x64 NTFS HD][Win8.1 x64 NTFS SSD][Linux x64 ext4 SSD][FreeBSD 10.1 ZFS SSD]]
  [[1][1][2592][2875][1198][29]]
  [[2][1][1284][2565][1344][25]]
  [[4][1][1420][2384][1327][35]]
  [[8][1][1262][1764][1254][55]]
  [[16][1][428][520][1260][37]]
]

Linux once against does a great job of O(1) to waiters complexity, but at the third of the speed of
a simple lock file and up to half the speed of Windows. Windows does better than Linux here especially
on SSDs where it is faster than a simple lock file, but doesn't scale to waiters once they pass CPU core count.
FreeBSD is two orders of magnitude slower which is because ZFS checksums and copy on writes file changes,
so every time we append 16 bytes we are forcing a full copy of the 128Kb extent to be issued. It would
appear that ZFS syncs its internal buffers when a different file descriptor atomic appends to the same file
__dash__ this has the above pathological performance outcome unfortunately.

This introduces the final potential solution which is that of the quagmire of advisory file locking.
This is an area where Windows and POSIX diverge very significantly, and the interactions between
Windows and POSIX when Windows locks regions in a file on a Samba share on a POSIX machine or when
POSIX does byte range locking at all (there is a very fun stanza in the POSIX standard which basically
releases all your locks on first file descriptor close) are full of quirks, races and other nasties.
For this reason you should avoid the very temporary and experimental code currently in AFIO which
implements Samba and NFS safe file range locking where theoretically both Windows and POSIX code
can safely lock ranges in files concurrently, those APIs are not documented for good reason!
Still, performance with these __dash__ despite the hoop jumping inefficiencies AFIO leaps through
to implement relatively sane semantics __dash__ is impressive.

[table:advisory_lock_file_performance Advisory lock file performance on various operating systems:
  [[writers][lock files][Win8.1 x64 NTFS HD][Win8.1 x64 NTFS SSD][Linux x64 ext4 SSD][FreeBSD 10.1 ZFS SSD]]
  [[1][1][5799][5166][3466][21536]]
  [[2][1][5788][6656][2215][11654]]
  [[4][1][5775][7020][1073][5384]]
  [[8][1][5773][6738][518][2584]]
  [[16][1][5695][5617][360][1326]]
]

Fascinatingly the tables suddenly switch here: Windows sees O(1) to waiters complexity, whilst
Linux and FreeBSD sees a mostly O(N) to waiters complexity drop in performance. FreeBSD, as with the
simple lock file performance, blows all others out of the water again in advisory lock performance too. I
should add here that because POSIX advisory locks are per process, the Linux and FreeBSD benchmarks were
generated by running N copies of the benchmark program whereas the NT kernel inherits the really
excellent and well thought through file byte range locking model of DEC VMS and treats locks as
effectively reference counted byte ranges, and therefore works as expected from a single process.
I have yet to add process-local byte range locking to simulate sane range locking for POSIX, so
expect the numbers above to worsen.

After all of that, we are left with this locking strategy matrix for __triplegit__:

[table:best_locking_strategy_matrix Fastest file system locking strategy for various operating systems:
  [[Operating system][Best locking policy]]
  [[Win8.1 x64 NTFS][Advisory locks fastest, then atomic append locks, finally lock files]]
  [[Linux x64 ext4][Lock files fastest, then advisory locks, finally atomic append locks]]
  [[FreeBSD 10.1 ZFS][Advisory locks fastest, then lock files, avoid atomic append locks at all costs]]
]

I should [*emphasise] once again that the advisory locking code is riddled with bugs and you should
not use it in your code at this time. Once I have a CI testing all possible combinations of locking
and nothing is erroring out I'll release that code for production use, probably in v1.4.

All these benchmarks came from this benchmarking program I wrote using AFIO which illustrates how
you might implement the techniques used above:

[benchmark_atomic_log]

'''<?dbhtml-include href="disqus_comments.html"?>'''

[endsect] [/atomic_logging]

[section:filesystem_races Handling races on the filing system]

'''<?dbhtml-include href="disqus_identifiers/filesystem_races.html"?>'''

Filing systems are a shared resource common to all processes on the system and sometimes the network,
and are therefore as a globally shared resource inherently racy. Yet overwhelmingly programs, even often those written
by world expert programmers, singularly assume
the filing system to be a static, constant and unchanging place only modifiable by the current program,
as indeed did until very recently the POSIX API standard which defines the common API for Linux, FreeBSD,
Mac OS X and other Unices.
When bug reports come in of data being lost, even very large professional corporations can make a real
hash of testing that their ["fix] isn't worse at losing data than the previous more naive implementation.
This is because when you program against a mental model of a static, unchanging filesystem you will become
inevitably surprised when it happens to change at exactly the wrong moment __dash__ which of course is
a moment you can never replicate on your developer workstation, thus making finding and fixing these
sorts of bug highly non-trivial.

In case you don't realise how much user data and productivity is lost each year to filing system races,
just look up ["corrupted Office file] on Google and weep. Even for us programmers, if you try keeping a
Git repository on a Samba drive expect some interesting, and moreover quite strongly associated to specific
combinations of client accessing the repo concurrently, object database corruption from time to time.

Well, there is some good news: AFIO makes maximum use of host OS filing system race safeguards, so if you
write your code against AFIO and take note of the race guarantees section in each individual per-API
reference documentation page, you should hopefully avoid any possibility of experiencing filing system
races.

[heading What AFIO provides for managing filing system raciness]

Firstly, readers will probably be quite surprised to learn that the only operating system capable of providing
completely race free filing system behaviour is Microsoft Windows, or rather the very well designed NT kernel API which AFIO uses directly.
Linux provides robust file descriptor path discovery and the `XXXat()` POSIX APIs, and with those AFIO can provide pretty
good race condition safety on Linux up to the final directory in the path.
Mac OS X provides an unfortunately quite broken file descriptor path discovery, and additionally does not provide the `XXXat()`
POSIX APIs and so AFIO cannot provide race protection, but can throw exceptions sometimes if it detects the filesystem
has suddenly changed and you're about to delete the wrong file (you shouldn't rely on this, it's racy). FreeBSD provides the `XXXat()` POSIX APIs, but its file
descriptor path discovery only works correctly for directory not file handles due to a kernel bug (I've opened
a feature request ticket for this at [@https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=198570 https://bugs.freebsd.org/bugzilla/show_bug.cgi?id=198570]) and therefore AFIO
can provide only race condition safety for directories only on FreeBSD.

Additionally, working with the filing system in a race safe way on POSIX requires opening a file descriptor to the
containing directory for every operation (some proprietary Linux extensions allow this to be avoided for some operations on
newer Linux kernels). AFIO will keep a global cache of open file handles for containing directories
on request using the `file_flags::hold_parent_open` flag which can be enabled per dispatcher or per individual file handle open,
this very significantly reduces the cost of race condition safety on POSIX ['for file entries only] as directories ignore
the `file_flags::hold_parent_open` flag, though at the cost of increased file descriptor usage, which has low hard limits
especially on OS X which is why it is disabled by default. The alternative if you don't want AFIO to bother with race safety
is to specify the `file_flags::no_race_protection` flag per dispatcher or per individual file handle open, this causes AFIO
to use the same maximum performance code paths as used before the v1.3 engine.

[heading How to implement filing system race safety on AFIO]

The essential rule for achieving maximum filing system race safety is to avoid using absolute paths where possible. If
you want your code to also be safe on POSIX, you must additionally only assume race safety up to the final directory in
a path __dash__ thereafter design your node to never behave racily within a single directory.

The core container type for specifying a location on the filing system to AFIO is __afio_path_op_req__ which looks like this:

    struct async_path_op_req
    {
        bool is_relative;              // Whether the precondition is also where this path begins.
        afio::path path;               // The filing system path to be used for this operation.
        file_flags flags;              // The flags to be used for this operation (note they can be overriden by flags passed during dispatcher construction).
        async_io_op precondition;      // An optional precondition for this operation.
        async_path_op_req(T &&);       // Converts T to a filesystem::path, makes it absolute, then converts to an afio::path
        async_path_op_req(bool, T &&); // If the bool is true, converts T to an afio::path fragment. If false, same as above overload (i.e. make absolute).
    };
    
For convenience, type markup is provided for the boolean taking constructor, these being `async_path_op_req::relative` and
`async_path_op_req::absolute`.

If the path is relative, then the path of the precondition is used as the base from which the relative path fragment
operates. On FreeBSD, Linux and Windows this base extension happens inside the kernel and so the current path of the precondition
really doesn't matter __dash__ it could be changing a thousand times per second and it wouldn't matter. On OS X due
to lack of the `XXXat()` POSIX APIs the path of the precondition is fetched and the extension done by hand.

An AFIO extension allows you to specify a file as precondition. In this situation, if you specify an empty path
then you mean the precondition itself which is very useful for deleting or renaming an open file handle. If you want
a sibling file, this can be found via a path fragment starting with `../`, though note that this necessarily is racy
(AFIO opens the containing directory of the file, ensuring the directory contains an inode matching the file, and
then uses that directory handle as a base __dash__ the race here being if the file relocates after matching its
containing directory).

[heading Gotchas specific to Microsoft Windows]

Finally, there are some gotchas specific to Microsoft Windows:

1. You cannot rename a directory which has an open file handle in any process to any item anywhere within itself or its children.

2. You cannot rename to a destination which has an open file handle with DELETE permissions (`file_flags::write`)
to itself or any of its parent directories in any process. You CAN do this from a source like this, but the destination cannot be
like this (why is this? It is not documented anywhere in Microsoft's documentation, but if I had to guess, I'd suggest that the
atomicity of the rename is implemented by taking an op lock on the destination, an op lock not granted if any handles exist which
could change the path suddenly. I'm not sure if Microsoft are themselves aware of this limitation).

One might note that much of the utility of race protecting APIs is lost with these restrictions. However, note that one could
emulate POSIX semantics by renaming out all the contents of a directory to be renamed to elsewhere, rename the directory, and then
renaming the contents back in. Given the pathetic slowness of opening handles on Windows, this might seem impossibly inefficient,
however NT provides a little known `FILE_DELETE_CHILD` permission which gives you delete/rename permission on all the children
and subchildren of a directory with just one handle open. I learned about this flag the hard way, by it breaking in many subtle ways
AFIO's functioning on Windows when it was requested by default, something which took several days of head scratching to track down. AFIO doesn't
currently do this trick of renaming out and back in on Windows, but might in the future after a lot more experimentation as to if
it is viable and reliable without surprises.

On Windows opening a directory with write access requests rename/delete privileges, whereas on POSIX
the write access request is ignored for directories as POSIX doesn't allow it anyway. This allows you to write identical code
which works universally.

As an example of some programming AFIO safely on an extremely unstable filing system, below is the functional test which verifies AFIO
for filing system race safety.
As you will see, a worker thread is solely dedicated to renaming directories to unique names whilst the main thread creates files
inside those constantly changing directories, and relinks them into another directory which is also constantly changing on POSIX, but
is stable on Windows. This is iterated for a substantial period of time to verify that nothing goes wrong.

[race_protection_example]

'''<?dbhtml-include href="disqus_comments.html"?>'''

[endsect] [/filesystem_races]


[section:so_what What performance benefit does asynchronous file i/o bring me? A demonstration]

'''<?dbhtml-include href="disqus_identifiers/so_what.html"?>'''

So we can schedule file i/o operations asynchronously with AFIO: what's the benefit of doing that
instead of creating separate threads of execution and executing the file i/o there instead?

As a quick summary as we're about to prove our case, there are three main benefits to using AFIO
over doing it by hand:

# You don't have to worry about threading or race conditions or losing error state (as much). AFIO
does all that for you.
# On platforms which provide native asynchronous file i/o support and/or native scatter/gather
file i/o support, AFIO will use that
instead of issuing multiple filing system operations using a thread pool to achieve
concurrency. This can very significantly reduce the number of threads needed to keep your
storage device fully occupied __dash__ remember that ['queue depth] i.e. that metric you see all
over storage device benchmarks is the number of operations in flight at once, which implies
the number of threads needed. Most storage devices are IOPS limited due to SATA or SAS
connection latencies without introducing queue depth __dash__ in particular, modern SSDs cannot
achieve tens of thousands of IOPS range without substantial queue depths, which without
using a native asynchronous file i/o API means lots of threads.
# It's very, very easy to have AFIO turn off file system caching, readahead or buffering
on a case by case basis which makes writing scalable random synchronous file i/o applications
much easier.

What these three items mean is that writing scalable high-performance filing system code is much easier.
Let us take a real world comparative example, this being a simple STL iostreams, Boost Filesystem and OpenMP
based find regular expression in files implementation:

[find_in_files_iostreams]

This implementation is fairly straightforward: enumerate all files in all directories below
the current directory into a vector, fire off N threads to open each file, read it entirely
into memory, regex it for the pattern and if found print the file's path.

Let's now look at the AFIO implementation, and you should prepare yourself because it is far
more mind bendy due to the many nestings of callbacks needed (it may remind you of WinRT or .NET
code, everything is asynchronous callback):

[warning In the v1.4 engine we ought to gain C++ 1z stackful coroutine support which will let me rewrite
the below to be vastly simpler and without the multitude of nested callback handlers.]

[find_in_files_afio]

Here the `find_in_files` class is used to carry state across the callbacks __dash__ one could just
as equally bind the state into each callback's parameters instead via some sort of pointer to
a struct. But the difference in complexity is noticeable __dash__ you're probably now asking, why
choose this hideous complexity over the much clearer OpenMP and iostreams implementation[footnote
Indeed many ask the same when programming WinRT apps __dash__ Microsoft's insistance on never allowing
any call to block can make simple designs explode with complexities of nested callbacks.]?

Well, that's simple: do you want maximum file i/o performance? Here is a search for ["Niall] in a
Boost working directory which contained about 7Gb of data across ~35k files[footnote Test machine
was a 3.5Ghz Intel i7-3770K Microsoft Windows 8 x64 machine with Seagate ST3320620AS 7200rpm hard
drive. Note that AFIO has a native WinIOCP backend which leverages host asynchronous file i/o support.]:

[table:find_in_files_performance Find in files performance for traditional vs AFIO implementations
  [[][iostreams, single threaded][iostreams, OpenMP][Boost.AFIO][Boost.AFIO `file_flags::os_direct`][Boost.AFIO `file_flags::os_mmap`[footnote The superiority of memory maps on Windows is because all buffered file i/o is done via memory copying to/from memory maps on Windows anyway, so eliminating that memory copy is huge.]]]
  [[Warm cache][812 Mb/sec][1810 Mb/sec][2663 Mb/sec][N/A][6512 Mb/sec]]
  [[][+0%][+123%][+228%][][+702%]]
  [[Cold cache[footnote File cache reset using [@http://technet.microsoft.com/en-us/sysinternals/ff700229.aspx]]][16 Mb/sec][[*8 Mb/sec]][15 Mb/sec][13.14 Mb/sec][24 Mb/sec]]
  [[][+0%][[*-50%]][-6%][-18%][+50%]]
]

Note how AFIO outperforms the OpenMP iostreams implementation by about 50% for a warm cache, with only
a 6% penalty for a cold cache over a single threaded implementation. Note the [*50%] penalty the OpenMP
iostreams implementation suffers for a cold cache __dash__ a naive multithreaded implementation causes
a lot of disc seeks. If you map a file into memory using `file_flags::os_mmap`, AFIO will `memcpy()` from
that map instead of reading __dash__ or of course you can use the map directly using the pointer returned
by `try_mapfile()`.

The eagle eyed amongst you will have noticed that the AFIO implementation looks hand tuned with a
special depth first algorithm balancing concurrency with seek locality __dash__ that's because I invested
two days into achieving maximum possible performance as a demonstration of AFIO's power (and to find
and fix some bottlenecks). Some might argue that this is therefore not a fair comparison to the OpenMP iostreams
implementation.

There are two parts to answering this argument. The first is that yes, the OpenMP iostreams search
algorithm is fairly stupid and simply tries to read as many files as there are CPUs, and those files could
be stored anywhere on the spinning disc. Because AFIO
can issue far more concurrent file open and read requests than OpenMP, it gives a lot more scope to
the filing system to optimise hard drive seeks and satisfy as many requests as is feasible __dash__
sufficiently so that with a cold cache, AFIO is a little slower than a single threaded iostreams
implementation where the filing system can spot the access pattern and prefetch quite effectively.
A completely valid solution to the OpenMP performance deficit would be to increase thread count dramatically.

The second part of answering that argument is this: AFIO's very flexible op chaining structure lets you
very easily twiddle with the execution dependency graph to achieve maximum possible performance by
balancing concurrency (too much or too little is slower, what you need is just the right balance)
and disc seeks (enumerations are best not done in parallel, it defeats any prefetching algorithm),
unlike the naive OpenMP implementation which is much harder to play around with. Don't get me wrong:
if you have plenty of time on your hands, you can implement a hand written and tuned find in files
implementation that is faster than AFIO's implementation, but it will have taken you a lot longer,
and the code will neither be as portable nor as maintainable.

'''<?dbhtml-include href="disqus_comments.html"?>'''

[endsect] [/so_what]

[endsect] [/ mini_programs]

[endsect] [/quickstart]
